# -*- coding: utf-8 -*-
"""DS 340W Coding

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1obrelZHnBv4wMmhuF0GLBY0Hzt4pbagu
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.pipeline import make_pipeline
from transformers import DistilBertTokenizer, DistilBertModel
from transformers import BertTokenizer, BertModel
import torch

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load a smaller multilingual model (DistilBERT)
tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased')
bert_model = BertModel.from_pretrained('distilbert-base-multilingual-cased').to(device)

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

from google.colab import drive

drive.mount('/content/drive')
df=pd.read_csv('/content/drive/MyDrive/DS 340W/data-augmented.csv', sep= ',', header = 0)

# Check missing values
df.isnull().sum()

df = df.dropna()

df.isnull().sum()

# Check spam and ham values
df['labels'].value_counts().plot(kind='bar')

# Initialize empty lists for each language
processed_data = []

# Languages to process
languages = ['text', 'text_es', 'text_ar', 'text_ru', 'text_pt']

# Define stopwords for each language
stopwords_ar = set(open('arabic.txt').read().splitlines())
stopwords_es = set(open('spanish.txt').read().splitlines())
stopwords_ru = set(open('russian.txt').read().splitlines())
stopwords_pt = set(open('portuguese.txt').read().splitlines())

# Create preprocessing function (modified to handle multiple languages)
def preprocess_text(text, language='english'):
    if pd.isna(text):  # Handle NaN values
        return ''

    # Convert to lowercase
    text = text.lower()

    # Language-specific preprocessing
    if language == 'english':
        # Remove special characters (keep apostrophes for contractions)
        text = re.sub(r"[^a-zA-Z\s']", '', text)
        stop_words = set(stopwords.words('english'))
    elif language == 'ar':
        # Arabic processing
        text = re.sub(r"[^\u0600-\u06FF\s]", '', text)  # Keep Arabic letters and spaces
        stop_words = stopwords_ar
    elif language == 'es':
        # Spanish processing
        text = re.sub(r"[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ\s]", '', text)
        stop_words = stopwords_es
    elif language == 'ru':
        # Russian processing
        text = re.sub(r"[^а-яА-ЯёЁ\s]", '', text)
        stop_words = stopwords_ru
    elif language == 'pt':
        # Portuguese processing
        text = re.sub(r"[^a-zA-ZáéíóúâêîôûãõçÁÉÍÓÚÂÊÎÔÛÃÕÇ\s]", '', text)
        stop_words = stopwords_pt
    else:
        # Default case (shouldn't happen with our languages list)
        text = re.sub(r"[^\w\s]", '', text)
        stop_words = set()

    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stopwords
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Join the tokens back into a single string
    preprocessed_text = ' '.join(filtered_tokens)

    return preprocessed_text

# Iterate over the DataFrame rows
for index, row in df.iterrows():
    label = row['labels']

    # Process each language column
    for lang in languages:
        text = row[lang]
        lang_code = 'english' if lang == 'text' else lang.split('_')[-1]
        processed_text = preprocess_text(text, lang_code)

        # Format as "label\tlanguage\tprocessed_text"
        formatted_row = f"{label}\t{lang}\t{processed_text}"
        processed_data.append(formatted_row)

# Create preprocessing function for the lines
def preprocess_line(data):
    preprocessed_data = []
    for line in data:
        parts = line.strip().split('\t')
        if len(parts) == 3:
            label, language, text = parts
            preprocessed_data.append([label, language, text])
    return preprocessed_data

preprocessed_data = preprocess_line(processed_data)

# Convert to DataFrame
processed_df = pd.DataFrame(preprocessed_data, columns=["labels", "language", "text"])

# Display the first few rows
print(processed_df.head())

# Encode labels
label_encoder = LabelEncoder()
df['labels'] = label_encoder.fit_transform(df['labels'])

# Function to encode texts in batches with progress tracking
def encode_texts_in_batches(texts, tokenizer, model, batch_size=32, max_length=128):
    encoded_features = []
    total_batches = (len(texts) // batch_size) + 1

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size].tolist()
        encoded_inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        ).to(device)

        with torch.no_grad():
            outputs = model(**encoded_inputs)

        # Use mean pooling of all token embeddings instead of just [CLS]
        batch_embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()
        encoded_features.append(batch_embeddings)

        # Print progress
        if (i // batch_size) % 10 == 0:
            print(f"Processed {min(i+batch_size, len(texts))}/{len(texts)} samples")

    return np.vstack(encoded_features)

# Languages to process
languages = ['text', 'text_es', 'text_ar', 'text_ru', 'text_pt']

# Dictionary to store features and results for each language
language_results = {}

# Initialize models with better parameters
model_configs = {
    "SVM": make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=42),
    "KNN": make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5)),
    "Logistic Regression": make_pipeline(StandardScaler(),
                                      LogisticRegression(max_iter=1000, solver='saga',
                                                        multi_class='auto', random_state=42)),
    "MLP": make_pipeline(StandardScaler(),
                        MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500,
                                     early_stopping=True, random_state=42))
}

# 1. Process each language individually
for lang in languages:
    print(f"\n{'='*50}")
    print(f"Processing language: {lang}")
    print(f"{'='*50}")

    # Feature extraction for current language
    X = encode_texts_in_batches(df[lang], tokenizer, bert_model)
    y = df['labels']

    # Split dataset
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Store results for this language
    language_results[lang] = {}

    # Train and evaluate models
    for name, model in model_configs.items():
        print(f"\nTraining {name} for {lang}...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        language_results[lang][name] = acc
        print(f"{name} Accuracy for {lang}: {acc:.4f}")

# 2. Process combined features from all languages
print(f"\n{'='*50}")
print("Processing combined features from all languages")
print(f"{'='*50}")

# Combine features from all languages (concatenate embeddings)
X_combined = np.concatenate([encode_texts_in_batches(df[lang], tokenizer, bert_model)
                            for lang in languages], axis=1)
y = df['labels']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y, test_size=0.2, random_state=42, stratify=y
)

# Store combined results
combined_results = {}

# Train and evaluate models on combined features
for name, model in model_configs.items():
    print(f"\nTraining {name} on combined features...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    combined_results[name] = acc
    print(f"{name} Accuracy for combined features: {acc:.4f}")

# Print summary of results
print("\n\n=== Summary of Results ===")
print("\nIndividual Language Results:")
for lang in languages:
    print(f"\n{lang}:")
    for model_name, acc in language_results[lang].items():
        print(f"  {model_name}: {acc:.4f}")

print("\nCombined Features Results:")
for model_name, acc in combined_results.items():
    print(f"  {model_name}: {acc:.4f}")